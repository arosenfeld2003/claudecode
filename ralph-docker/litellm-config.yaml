# LiteLLM Configuration for Ralph Docker
# Translates API calls to Ollama format
# Docs: https://docs.litellm.ai/docs/providers/ollama

model_list:
  # Qwen 2.5 Coder models (recommended for coding tasks)
  - model_name: ollama/qwen2.5-coder:32b
    litellm_params:
      model: ollama/qwen2.5-coder:32b
      api_base: http://host.docker.internal:11434

  - model_name: ollama/qwen2.5-coder:14b
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://host.docker.internal:11434

  - model_name: ollama/qwen2.5-coder:7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://host.docker.internal:11434

  # DeepSeek Coder
  - model_name: ollama/deepseek-coder-v2
    litellm_params:
      model: ollama/deepseek-coder-v2
      api_base: http://host.docker.internal:11434

  - model_name: ollama/deepseek-coder-v2:16b
    litellm_params:
      model: ollama/deepseek-coder-v2:16b
      api_base: http://host.docker.internal:11434

  # CodeLlama
  - model_name: ollama/codellama:34b
    litellm_params:
      model: ollama/codellama:34b
      api_base: http://host.docker.internal:11434

  - model_name: ollama/codellama:13b
    litellm_params:
      model: ollama/codellama:13b
      api_base: http://host.docker.internal:11434

  - model_name: ollama/codellama:7b
    litellm_params:
      model: ollama/codellama:7b
      api_base: http://host.docker.internal:11434

  # Llama 3.x
  - model_name: ollama/llama3.1:70b
    litellm_params:
      model: ollama/llama3.1:70b
      api_base: http://host.docker.internal:11434

  - model_name: ollama/llama3.1:8b
    litellm_params:
      model: ollama/llama3.1:8b
      api_base: http://host.docker.internal:11434

  - model_name: ollama/llama3.2:3b
    litellm_params:
      model: ollama/llama3.2:3b
      api_base: http://host.docker.internal:11434

  # Mistral / Mixtral
  - model_name: ollama/mixtral:8x7b
    litellm_params:
      model: ollama/mixtral:8x7b
      api_base: http://host.docker.internal:11434

  - model_name: ollama/mistral:7b
    litellm_params:
      model: ollama/mistral:7b
      api_base: http://host.docker.internal:11434

litellm_settings:
  # Drop unsupported params instead of erroring
  drop_params: true
  # Enable streaming
  set_verbose: false

general_settings:
  # Master key for local use (required for API access)
  master_key: sk-ralph-local
